{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a Spark session available in our notebook as the `spark` object. In the background, YARN allocate CPU and RAM resources to create a driver and one or more executor JVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://slalomdsvm:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2.3.1.4.0-315</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa1bc12bb70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a spark-session (akin to what pyspark provides when it is started)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the YARN UI: [http://slalomdsvm:8088/ui2/](http://slalomdsvm:8088/ui2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/08/07 15:52:28 INFO client.RMProxy: Connecting to ResourceManager at slalomdsvm/10.0.2.15:8050\n",
      "20/08/07 15:52:28 INFO client.AHSProxy: Connecting to Application History server at slalomdsvm/10.0.2.15:10200\n",
      "Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n",
      "application_1596825752622_0004\t       pyspark-shell\t               SPARK\t   vagrant\t   default\t           RUNNING\t         UNDEFINED\t            10%\t             http://slalomdsvm:4040\n"
     ]
    }
   ],
   "source": [
    "! yarn application -list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Spark session options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.driver.appUIAddress', 'http://slalomdsvm:4040'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.memory', '512M'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.history.fs.cleaner.interval', '7d'),\n",
       " ('spark.shuffle.io.serverThreads', '128'),\n",
       " ('spark.yarn.historyServer.address', 'slalomdsvm:18081'),\n",
       " ('spark.sql.streaming.streamingQueryListeners', ''),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.sql.statistics.fallBackToHdfs', 'true'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.app.id', 'application_1596825752622_0004'),\n",
       " ('spark.shuffle.file.buffer', '1m'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.hive.convertMetastoreOrc', 'true'),\n",
       " ('spark.yarn.dist.files', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '26214400'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.driver.extraClassPath', ''),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.sql.hive.metastore.jars',\n",
       "  '/usr/hdp/current/spark2-client/standalone-metastore/*'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://slalomdsvm:8088/proxy/application_1596825752622_0004'),\n",
       " ('spark.driver.port', '45364'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'slalomdsvm'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1596825752622_0004'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.memory', '1G'),\n",
       " ('spark.sql.orc.impl', 'native'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.sql.queryExecutionListeners', ''),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.io.compression.lz4.blockSize', '128kb'),\n",
       " ('spark.driver.host', 'slalomdsvm'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.history.fs.cleaner.maxAge', '90d'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.orc.filterPushdown', 'true'),\n",
       " ('spark.shuffle.io.backLog', '8192'),\n",
       " ('spark.extraListeners', ''),\n",
       " ('spark.unsafe.sorter.spill.reader.buffer.size', '1m'),\n",
       " ('spark.shuffle.unsafe.file.output.buffer', '5m'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.executor.cores', '1'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseNUMA'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.warehouse.dir', '/apps/spark/warehouse'),\n",
       " ('spark.history.store.path', '/var/lib/spark2/shs_db'),\n",
       " ('spark.sql.hive.metastore.version', '3.0')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and see dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['id', 'first_name', 'last_name']\n",
    "rows = [\n",
    "    (1, 'John', 'Doe'),\n",
    "    (1, 'John', 'Doe'), \n",
    "    (1, 'John', None), \n",
    "    (2, 'Jane', 'Doe'),\n",
    "    (3, 'Herbie', 'Hancock'),\n",
    "    (4, 'Erin', 'brockovich'),        \n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(rows, col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['id', 'number_sox']\n",
    "rows = [\n",
    "    (1, 24),\n",
    "    (2, 30),\n",
    "    (3, 29),\n",
    "    (4, 40),        \n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(rows, col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- number_sox: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the DF:\n",
    "\n",
    "  * Asking for `df1` only shows the reference to the object. This is beacuse of Spark's lazy evaluation: the dataframe has not been created yet (`createDataFrame()` is a transformation).\n",
    "  * The dataframe will only be created when we apply an action to it like `show()`.\n",
    "  * Note the difference in duration between the 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, first_name: string, last_name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|first_name| last_name|\n",
      "+---+----------+----------+\n",
      "|  1|      John|       Doe|\n",
      "|  1|      John|       Doe|\n",
      "|  1|      John|      null|\n",
      "|  2|      Jane|       Doe|\n",
      "|  3|    Herbie|   Hancock|\n",
      "|  4|      Erin|brockovich|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, number_sox: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|number_sox|\n",
      "+---+----------+\n",
      "|  1|        24|\n",
      "|  2|        30|\n",
      "|  3|        29|\n",
      "|  4|        40|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  1|      John|       Doe|        24|\n",
      "|  1|      John|       Doe|        24|\n",
      "|  1|      John|      null|        24|\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df1.join(df2, 'id', how = 'inner')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  1|      John|       Doe|        24|\n",
      "|  1|      John|      null|        24|\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.distinct() \\\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing rows containing `null` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  1|      John|       Doe|        24|\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a column object with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'id'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|first_name|\n",
      "+---+----------+\n",
      "|  1|      John|\n",
      "|  3|    Herbie|\n",
      "|  2|      Jane|\n",
      "|  4|      Erin|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_df = df.select('id', 'first_name')\n",
    "\n",
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  1|      John|       Doe|        24|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter((df.number_sox <= 25) | (df.number_sox >= 35))\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomizing and ordering rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  1|      John|       Doe|        24|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "randomized_df = df.orderBy(rand())\n",
    "randomized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  1|      John|       Doe|        24|\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomized_df \\\n",
    ".orderBy('id') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  1|      John|       Doe|        24|\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomized_df \\\n",
    ".orderBy('last_name', 'number_sox') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|first_name| last_name|number_sox|\n",
      "+---+----------+----------+----------+\n",
      "|  2|      Jane|       Doe|        30|\n",
      "|  1|      John|       Doe|        24|\n",
      "|  3|    Herbie|   Hancock|        29|\n",
      "|  4|      Erin|brockovich|        40|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "randomized_df \\\n",
    ".orderBy('last_name', desc('number_sox')) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDFs) and Row-wise operations (\"Lamda\" functions)\n",
    "\n",
    "UDFs can be defined to be re-used during row-wise operations (ex: Lambda functions in Pandas) on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your function\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# Define the UDF using a \"classic\" function\n",
    "def sox_pairs(x):\n",
    "    return round(float(x)/2.0)\n",
    "    \n",
    "sox_pairs_udf = udf(lambda x: sox_pairs(x), IntegerType())\n",
    "\n",
    "# OR \n",
    "# Define the UDF using a lambda function\n",
    "sox_pairs_udf_l = udf(lambda x: round(float(x)/2.0), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+---------------+\n",
      "| id|first_name| last_name|number_sox|number_sox_pair|\n",
      "+---+----------+----------+----------+---------------+\n",
      "|  1|      John|       Doe|        24|             12|\n",
      "|  3|    Herbie|   Hancock|        29|             14|\n",
      "|  2|      Jane|       Doe|        30|             15|\n",
      "|  4|      Erin|brockovich|        40|             20|\n",
      "+---+----------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the UDFs to the DF\n",
    "\n",
    "df \\\n",
    ".withColumn(\"number_sox_pair\", sox_pairs_udf(\"number_sox\")) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+---------------+\n",
      "| id|first_name| last_name|number_sox|number_sox_pair|\n",
      "+---+----------+----------+----------+---------------+\n",
      "|  1|      John|       Doe|        24|             12|\n",
      "|  3|    Herbie|   Hancock|        29|             14|\n",
      "|  2|      Jane|       Doe|        30|             15|\n",
      "|  4|      Erin|brockovich|        40|             20|\n",
      "+---+----------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_pairs = df \\\n",
    ".withColumn(\"number_sox_pair\", sox_pairs_udf_l(\"number_sox\"))\n",
    "\n",
    "df_with_pairs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The concept of UDF can be taken one step futher to be applicable to aggregation situation, these UDFs are called UDAFs (User Defined Aggregated Functions). Ex: if you want to apply an advanced algorithm to a dataframe in an aggregated fashion: you would need to create a UDAF which implements the algorithm but is also taking care of grouping. This is an expert topic and is not covered in this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "| last_name|number_sox_per_family|\n",
      "+----------+---------------------+\n",
      "|   Hancock|                   29|\n",
      "|       Doe|                   54|\n",
      "|brockovich|                   40|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "counts_df = df  \\\n",
    ".groupBy('last_name') \\\n",
    ".agg(sum('number_sox') \\\n",
    ".alias('number_sox_per_family'))\n",
    "\n",
    "counts_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding a \"list\" column into rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a column contains a list, it can be expanded into multiple rows, one row per item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  list|\n",
      "+---+------+\n",
      "|  1|[A, B]|\n",
      "|  2|   [C]|\n",
      "|  3|[D, D]|\n",
      "|  4|[E, F]|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['id', 'list']\n",
    "rows = [\n",
    "    (1, ['A', 'B']),\n",
    "    (2, ['C']),\n",
    "    (3, ['D', 'D']),\n",
    "    (4, ['E', 'F']),        \n",
    "]\n",
    "\n",
    "list_df = spark.createDataFrame(rows, col_names)\n",
    "list_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|item|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  1|   B|\n",
      "|  2|   C|\n",
      "|  3|   D|\n",
      "|  3|   D|\n",
      "|  4|   E|\n",
      "|  4|   F|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_df = list_df.select('id', explode('list').alias('item'))\n",
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsing multiple rows into a \"list\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversly, multiple rows can be collapsed into a list or a set with one row per list or set. Order is conserved for  lists, sets do have the concept of order so the original ordering information from the rows will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  list|\n",
      "+---+------+\n",
      "|  1|[A, B]|\n",
      "|  3|[D, D]|\n",
      "|  2|   [C]|\n",
      "|  4|[E, F]|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "exploded_df \\\n",
    ".groupBy('id') \\\n",
    ".agg(collect_list('item') \\\n",
    ".alias('list')) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|   set|\n",
      "+---+------+\n",
      "|  1|[B, A]|\n",
      "|  3|   [D]|\n",
      "|  2|   [C]|\n",
      "|  4|[F, E]|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "exploded_df \\\n",
    ".groupBy('id') \\\n",
    ".agg(collect_set('item') \\\n",
    ".alias('set')) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from a CSV file on HDFS into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|1849-01-01 00:00:00|            26.704|                        1.435|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-02-01 00:00:00|            27.434|                        1.362|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-03-01 00:00:00|            28.101|                        1.612|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-04-01 00:00:00|             26.14|           1.3869999999999998|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-05-01 00:00:00|            25.427|                          1.2|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-06-01 00:00:00|            24.844|                        1.402|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-07-01 00:00:00|24.058000000000003|                        1.254|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-08-01 00:00:00|            23.576|                        1.265|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-09-01 00:00:00|            23.662|                        1.226|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-10-01 00:00:00|            25.263|                        1.175|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-11-01 00:00:00|26.331999999999997|                        1.507|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1849-12-01 00:00:00|             25.45|                        1.838|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-01-01 00:00:00|            25.803|                        1.943|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-02-01 00:00:00|             27.89|                         1.43|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-03-01 00:00:00|            27.852|                        2.173|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-04-01 00:00:00|26.546999999999997|                        1.662|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-05-01 00:00:00|            25.379|                        1.355|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-06-01 00:00:00|24.903000000000002|                        1.178|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-07-01 00:00:00|24.040000000000006|                        1.301|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "|1850-08-01 00:00:00|23.758000000000003|           1.2819999999999998|Abidjan|Côte D'Ivoire|   5.63N|    3.23W|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/user/vagrant/data/earth-surface-temperature/csv/GlobalLandTemperaturesByMajorCity.csv\"\n",
    "\n",
    "temperature_df = spark \\\n",
    ".read \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferschema\", \"true\") \\\n",
    ".option(\"mode\", \"DROPMALFORMED\") \\\n",
    ".csv(csv_path)\n",
    "\n",
    "temperature_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239177"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23828"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df = temperature_df.sample(fraction = 0.1, seed = 1234)\n",
    "sampled_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a Spark dataframe to a parquet file on HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a great format to persist tabular data. It performs especially well for dataframes which have columns with values repeating on contigous rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"/user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/08/07 15:53:29 INFO fs.TrashPolicyDefault: Moved: 'hdfs://slalomdsvm:8020/user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity' to trash at: hdfs://slalomdsvm:8020/user/vagrant/.Trash/Current/user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity1596840809338\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -R $parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default write, will write to multiple files (due to number reducers in Spark's internal architecture)\n",
    "temperature_df \\\n",
    ".write \\\n",
    ".parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/_SUCCESS\r\n",
      "-rw-r--r--   1 vagrant hdfs    854.8 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/part-00000-14911455-fc0f-4379-8b5a-a208d3121de6-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs    567.4 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/part-00001-14911455-fc0f-4379-8b5a-a208d3121de6-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls -R -h $parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control the number of output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 1\n",
    "\n",
    "temperature_df \\\n",
    ".coalesce(n_files) \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/_SUCCESS\r\n",
      "-rw-r--r--   1 vagrant hdfs      1.2 M 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/part-00000-2a38e5fa-86ee-41ff-be6a-eec3e97e6d7a-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls -R -h $parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet also provides a very convenient partitioning functionality, the data for each single value in the partition will be under its own directory on HDFS.\n",
    "In-file indexing  is comping up but requires installation of additional libraries and is not ubiquitous yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df \\\n",
    ".orderBy('Country', 'City', 'dt') \\\n",
    ".write \\\n",
    ".partitionBy('Country') \\\n",
    ".mode('overwrite') \\\n",
    ".parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Afghanistan\r\n",
      "-rw-r--r--   1 vagrant hdfs     18.4 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Afghanistan/part-00000-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     17.8 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Afghanistan/part-00001-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "drwxr-xr-x   - vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Angola\r\n",
      "-rw-r--r--   1 vagrant hdfs      2.4 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Angola/part-00001-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     17.3 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Angola/part-00002-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     12.4 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Angola/part-00003-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "drwxr-xr-x   - vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Australia\r\n",
      "-rw-r--r--   1 vagrant hdfs     10.7 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Australia/part-00003-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     18.8 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Australia/part-00004-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     20.2 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Australia/part-00005-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     16.7 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Australia/part-00006-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "drwxr-xr-x   - vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Bangladesh\r\n",
      "-rw-r--r--   1 vagrant hdfs      2.6 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Bangladesh/part-00006-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     19.6 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Bangladesh/part-00007-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     18.7 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Bangladesh/part-00008-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs      3.3 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Bangladesh/part-00009-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "drwxr-xr-x   - vagrant hdfs          0 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Brazil\r\n",
      "-rw-r--r--   1 vagrant hdfs     18.2 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Brazil/part-00009-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 vagrant hdfs     18.5 K 2020-08-07 15:53 /user/vagrant/data/earth-surface-temperature/parquet/GlobalLandTemperaturesByMajorCity/Country=Brazil/part-00010-8ddc7980-029a-4092-887c-7630122c97e7.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls -R -h $parquet_path | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: In general, we want to create files with a size of a few times the HDFS block-size (default: 128MB). We want a few large files (YES: ≈100+MB -> ≈1GB), not many small files (NO: ≈1KB -> ≈10MB). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting a Spark dataframe into a \"regular\" pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, first_name='John', last_name='Doe', number_sox=24),\n",
       " Row(id=3, first_name='Herbie', last_name='Hancock', number_sox=29),\n",
       " Row(id=2, first_name='Jane', last_name='Doe', number_sox=30),\n",
       " Row(id=4, first_name='Erin', last_name='brockovich', number_sox=40)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect as a list of pyspark.sql.Rows\n",
    "collected = df.collect()\n",
    "collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>number_sox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Herbie</td>\n",
       "      <td>Hancock</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Doe</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Erin</td>\n",
       "      <td>brockovich</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name   last_name  number_sox\n",
       "0   1       John         Doe          24\n",
       "1   3     Herbie     Hancock          29\n",
       "2   2       Jane         Doe          30\n",
       "3   4       Erin  brockovich          40"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect into a \"classic\" pandas dataframe\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing the spak session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the `spark` session to destroy the release cluster CPU/RAM resources (completes YARN application and destroys executors, driver JVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the YARN UI: [http://slalomdsvm:8088/ui2/](http://slalomdsvm:8088/ui2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/08/07 15:59:54 INFO client.RMProxy: Connecting to ResourceManager at slalomdsvm/10.0.2.15:8050\n",
      "20/08/07 15:59:55 INFO client.AHSProxy: Connecting to Application History server at slalomdsvm/10.0.2.15:10200\n",
      "Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n"
     ]
    }
   ],
   "source": [
    "! yarn application -list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
